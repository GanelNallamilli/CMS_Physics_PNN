{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68505eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#%matplotlib nbagg\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "df = pd.read_parquet(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\merged_nominal.parquet')\n",
    "\n",
    "\n",
    "with open(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\summary.json', \"r\") as f:\n",
    "  proc_dict = json.load(f)[\"sample_id_map\"]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3be820",
   "metadata": {},
   "outputs": [],
   "source": [
    "signalnames = ['GluGluToRadionToHHTo2G2Tau_M-260',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-270',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-280',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-290',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-300',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-320',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-350',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-400',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-450',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-500',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-550',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-600',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-650',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-700',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-750',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-800',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-900',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-1000']\n",
    "signalnames.reverse()\n",
    "\n",
    "TopFeatures=['Diphoton_pt_mgg','weight_central','Classification']\n",
    "epochs=200\n",
    "aucscoreslist=[]\n",
    "name=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48a0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpla\\AppData\\Local\\Temp\\ipykernel_11988\\2652996573.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epoch 1: Loss = 7.091320730978623e-05\n",
      "For Epoch 2: Loss = 6.846606993349269e-05\n",
      "For Epoch 3: Loss = 6.623139779549092e-05\n",
      "For Epoch 4: Loss = 6.420008139684796e-05\n",
      "For Epoch 5: Loss = 6.23715459369123e-05\n",
      "For Epoch 6: Loss = 6.0738362662959844e-05\n",
      "For Epoch 7: Loss = 5.926972153247334e-05\n",
      "For Epoch 8: Loss = 5.793385935248807e-05\n",
      "For Epoch 9: Loss = 5.670267637469806e-05\n",
      "For Epoch 10: Loss = 5.5549811804667115e-05\n",
      "For Epoch 11: Loss = 5.44537506357301e-05\n",
      "For Epoch 12: Loss = 5.339824201655574e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 181>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m lossdata\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 190\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #signalname=signalnames[name]\n",
    "# signalname='GluGluToRadionToHHTo2G2Tau_M-1000'\n",
    "\n",
    "# sig = df[df.process_id == proc_dict[f\"{signalname}\"]] # just one signal process, mass of X is 1000 GeV\n",
    "# sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n",
    "# \"\"\"Concatenating the background data\"\"\"\n",
    "# background_list=['Data','DiPhoton', 'TTGG', 'TTGamma',#list of each bkgs for concatenation\n",
    "#  'TTJets',\n",
    "#  'VBFH_M125',\n",
    "#  'VH_M125',\n",
    "#  'WGamma',\n",
    "#  'ZGamma',\n",
    "#  'ggH_M125', \n",
    "#  'ttH_M125',\n",
    "#  'GJets']\n",
    "\n",
    "# listforconc=[]\n",
    "# for i in background_list:                               #creating a concatenated list of bkg\n",
    "#     bkgg = df[df.process_id == proc_dict[i]]\n",
    "#     listforconc.append(bkgg)\n",
    "\n",
    "# background = pd.concat(listforconc)\n",
    "# background['Classification']=np.zeros(background['Diphoton_mass'].size)\n",
    "\n",
    "# \"\"\"The features requiring exclusion of -9 values\"\"\"\n",
    "# MinusNineBinning=['ditau_met_dPhi',\n",
    "#                   'ditau_deta',\n",
    "#                   'ditau_dR',\n",
    "#                   'ditau_dphi',\n",
    "#                   'ditau_pt',\n",
    "#                   'Diphoton_ditau_dphi',\n",
    "#                   'dilep_leadpho_mass',\n",
    "#                   'reco_MX_mgg',\n",
    "#                   'Diphoton_ditau_deta',\n",
    "#                   'Diphoton_sublead_lepton_deta',\n",
    "#                   'Diphoton_sublead_lepton_dR',\n",
    "#                   'LeadPhoton_ditau_dR',\n",
    "#                   'ditau_mass']\n",
    "\n",
    "# \"\"\"Concatenating Signal and Background\"\"\"\n",
    "# \"\"\"Choosing Best Features given the M=1000 AUC scores\"\"\"\n",
    "# FullSignalBackground=pd.concat([sig,background])\n",
    "\n",
    "# df_TopFeatures=pd.DataFrame()\n",
    "\n",
    "# #   TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "# # includes classification and weights\n",
    "# #TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','weight_central','Classification']\n",
    "# #    TopFeatures=['reco_MX_mgg','weight_central','Classification']\n",
    "\n",
    "# \"\"\"A dataset consisting of only the essential features\"\"\"\n",
    "# for feature in TopFeatures:\n",
    "#     df_TopFeatures[feature]=FullSignalBackground[feature]\n",
    "\n",
    "# \"\"\"Removal of the values that are binned at -9 from the necessary features\"\"\"\n",
    "# for columns in df_TopFeatures.columns:\n",
    "#     if columns in MinusNineBinning:\n",
    "#         df_TopFeatures = df_TopFeatures.loc[(df_TopFeatures[columns] > -8)]\n",
    "\n",
    "# df_TopFeatures = df_TopFeatures.sample(frac=1, random_state=42)  # Setting frac=1 shuffles all rows\n",
    "\n",
    "# features = df_TopFeatures # Extracting features\n",
    "\n",
    "# labels = df_TopFeatures['Classification']  # Extracting labels\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=(1/3), random_state=42)\n",
    "# test_weights=pd.DataFrame()\n",
    "# train_weights=pd.DataFrame()\n",
    "\n",
    "# weightofsignal=train_features[train_features['Classification']==1]['weight_central'].sum()\n",
    "# weightofbackground=train_features[train_features['Classification']==0]['weight_central'].sum()\n",
    "# scale=weightofsignal/weightofbackground\n",
    "\n",
    "# \"\"\"reweighting the weight_central column in entire data \n",
    "# set such that for background and signal \"\"\"\n",
    "# train_features.loc[train_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "# test_features.loc[test_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "\n",
    "\n",
    "# train_weights['weight_central']=train_features['weight_central']\n",
    "# test_weights['weight_central']=test_features['weight_central']\n",
    "\n",
    "\n",
    "\n",
    "# train_features = train_features.drop(columns=['weight_central'])\n",
    "# train_features = train_features.drop(columns=['Classification'])\n",
    "# test_features=test_features.drop(columns=['weight_central'])\n",
    "\n",
    "# train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "# train_weights_tensor = torch.tensor(train_weights.values,dtype=torch.float32)\n",
    "# train_labels_tensor = torch.tensor(train_labels.values,dtype=torch.float32)\n",
    "\n",
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleNN,self).__init__()\n",
    "#         self.hidden1 = nn.Linear(1, 8)\n",
    "#         self.act1 = nn.ReLU()\n",
    "# #         self.hidden2 = nn.Linear(20, 8)\n",
    "# #         self.act2 = nn.ReLU()\n",
    "# #         self.output = nn.Linear(8, 1)\n",
    "# #         self.hidden1 = nn.Linear(16, 8)\n",
    "# #         self.act1 = nn.ReLU()\n",
    "# #         self.hidden2 = nn.Linear(20, 40)\n",
    "# #         self.act2 = nn.ReLU()\n",
    "# #         self.hidden3 = nn.Linear(40, 16)\n",
    "# #         self.act3 = nn.ReLU()\n",
    "# #         self.hidden4 = nn.Linear(16, 8)\n",
    "# #         self.act4 = nn.ReLU()\n",
    "#         self.output = nn.Linear(8, 1)\n",
    "#         self.act_output = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.act1(x)\n",
    "#       #  x = self.hidden2(x)\n",
    "#       #  x = self.act2(x)\n",
    "#       #  x = self.hidden3(x)\n",
    "#       #  x = self.act3(x)\n",
    "#       #  x = self.hidden4(x)\n",
    "#       #  x = self.act4(x)\n",
    "#         x = self.output(x)\n",
    "#         x = self.act_output(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def weightedBCELoss(self, input, target, weight):\n",
    "#       x, y, w = input, target, weight\n",
    "#       log = lambda x: torch.log(x*(1-1e-8) + 1e-8)\n",
    "#       #return torch.mean(-w * (y*log(x) + (1-y)*log(1-x)))\n",
    "#       return -w * (y*log(x) + (1-y)*log(1-x))\n",
    "\n",
    "#     def batch_weightedBCELoss(self, input, target, weight, batch_size):\n",
    "# #batch_weightedBCELoss(self, train, train_labels_tensor, train_weights_tensor, batch_size)\n",
    "#         self.batch_size=batch_size\n",
    "\n",
    "#         target=target.unsqueeze(1)\n",
    "\n",
    "\n",
    "#        # train=model.forward(input)\n",
    "\n",
    "#         total_batch_err=torch.empty(0,1)\n",
    "#         output_length=input.shape[0]\n",
    "#         batch_remainder=output_length%batch_size\n",
    "\n",
    "#         for i in range(0, output_length//batch_size):\n",
    "#             weights = weight[i*(batch_size):(i+1)*(batch_size), :]\n",
    "#             labels = target[i*(batch_size):(i+1)*(batch_size), :]\n",
    "#             inputs = input[i*(batch_size):(i+1)*(batch_size), :]\n",
    "\n",
    "#             loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "#             total_batch_err=torch.cat((total_batch_err,loss)) \n",
    "#         #    print(total_batch_err.shape[0])\n",
    "\n",
    "#         if batch_remainder > 0:\n",
    "#             weights = weight[(output_length//batch_size)*batch_size:, :]\n",
    "#             labels = target[(output_length//batch_size)*batch_size:, :]\n",
    "#             inputs = input[(output_length//batch_size)*batch_size:, :]\n",
    "\n",
    "#             loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "#             #weights = train_weights_tensor[(train_weights_tensor.shape[0]//batch_size)*batch_size:, :]\n",
    "#             total_batch_err=torch.cat((total_batch_err,loss))\n",
    "#         #    print(total_batch_err.shape[0])\n",
    "\n",
    "#         return torch.mean(total_batch_err)\n",
    "\n",
    "\n",
    "# model = SimpleNN()\n",
    "# lr=0.01\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer.zero_grad()\n",
    "# lossdata=[]\n",
    "# df_Prediction=pd.DataFrame()\n",
    "# #    epochs=200\n",
    "# epochlist=[]\n",
    "# for i in range(1,epochs+1):\n",
    "#     epochlist.append(i)\n",
    "# trained_data_list = []\n",
    "# for i in range(0,epochs):\n",
    "#     trained=model.forward(train_features_tensor)\n",
    "#     trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "#   #  trained_data_list.append(trained_data.copy())  # Append trained_data to the list\n",
    "    \n",
    "#     df_Prediction[f'Epoch {i}'] = trained_data\n",
    "#     loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "#     lossdata.append(loss.item())\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "# #df_Prediction = pd.concat(trained_data_list, axis=1)\n",
    "# #df_Prediction.columns = [f'Epoch {i}' for i in range(epochs)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # figure=plt.figure()\n",
    "# # plt.plot(epochlist,lossdata)\n",
    "# # plt.xlabel('Epoch')\n",
    "# # plt.ylabel('Loss')\n",
    "# # plt.show()    \n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Plot of Loss vs Epoch\n",
    "# \"\"\"\n",
    "\n",
    "# # figure=plt.figure()\n",
    "# # plt.plot(epochlist,lossdata)\n",
    "# # plt.xlabel('Epoch')\n",
    "# # plt.ylabel('Loss')\n",
    "# # plt.show()    \n",
    "\n",
    "# #epoch_= 'Epoch 99'\n",
    "# \"\"\"Adding the labels to the prediction dataframe\"\"\"\n",
    "# #epoch_= 'Epoch 99'\n",
    "# epoch_=f'Epoch {epochs-1}'\n",
    "# train_labels_=pd.DataFrame({'Classification': train_labels}).reset_index(drop=True)\n",
    "# df_Prediction = pd.concat([df_Prediction, train_labels_], axis=1)\n",
    "# epoch_=f'Epoch {epochs-1}'\n",
    "# # plt.figure()\n",
    "# # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "# # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "# # plt.legend()\n",
    "# # plt.xlabel('Classification of Events')\n",
    "# # plt.ylabel('Number of Events')\n",
    "# # plt.title('Comparison of the expected output and the trained output')\n",
    "# # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "# # plt.show()\n",
    "\n",
    "# df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "# fpr, tpr, thresholds = roc_curve(df_Prediction['Classification'], df_Prediction[epoch_])\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# fig=plt.figure()    \n",
    "# plt.plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "# plt.xlabel('FPR')\n",
    "# plt.ylabel('TPR')\n",
    "# plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # #epoch_= 'Epoch 99'\n",
    "# # epoch_=f'Epoch {epochs-1}'\n",
    "# # plt.figure()\n",
    "# # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "# # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "# # plt.legend()\n",
    "# # plt.xlabel('Classification of Events')\n",
    "# # plt.ylabel('Number of Events')\n",
    "# # plt.title('Comparison of the expected output and the trained output')\n",
    "# # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "# # Plot 1: Line plot (plt.plot)\n",
    "# axs[1].plot(epochlist, lossdata)\n",
    "# axs[1].set_xlabel('Epoch')\n",
    "# axs[1].set_ylabel('Loss')\n",
    "# axs[1].set_title('Loss per Epoch')\n",
    "\n",
    "# # Plot 2: Histograms (two separate subplots)\n",
    "\n",
    "# axs[0].set_title('Trained Histogram')\n",
    "\n",
    "# axs[0].set_xlabel('Trained Classification')\n",
    "# axs[0].set_ylabel('Number Events')\n",
    "\n",
    "# axs[0].hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\",range=(0,1))\n",
    "# axs[0].hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background',range=(0,1))\n",
    "# axs[0].legend()\n",
    "# axs[0].set_title('Expected Histogram')\n",
    "\n",
    "# # Plot 3: Scatter plot with AUC score\n",
    "# # axs[2].scatter(FPR_arr, TPR_arr, label=f'AUC: {AUCscore}')\n",
    "# # axs[2].set_xlabel('FPR')\n",
    "# # axs[2].set_ylabel('TPR')\n",
    "# # axs[2].legend()\n",
    "# # axs[2].set_title('ROC Curve')\n",
    "\n",
    "# axs[2].plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "# axs[2].set_xlabel('FPR')\n",
    "# axs[2].set_ylabel('TPR')\n",
    "# axs[2].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "# axs[2].legend()\n",
    "\n",
    "\n",
    "# # Adjust layout and display\n",
    "# plt.tight_layout()\n",
    "# #plt.savefig(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_1\")\n",
    "# print(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_\")\n",
    "\n",
    "# plt.show()\n",
    "# print(roc_auc)\n",
    "# aucscoreslist.append(roc_auc)\n",
    "# name+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f8215c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch 0</th>\n",
       "      <th>Epoch 1</th>\n",
       "      <th>Epoch 2</th>\n",
       "      <th>Epoch 3</th>\n",
       "      <th>Epoch 4</th>\n",
       "      <th>Epoch 5</th>\n",
       "      <th>Epoch 6</th>\n",
       "      <th>Epoch 7</th>\n",
       "      <th>Epoch 8</th>\n",
       "      <th>Epoch 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Epoch 191</th>\n",
       "      <th>Epoch 192</th>\n",
       "      <th>Epoch 193</th>\n",
       "      <th>Epoch 194</th>\n",
       "      <th>Epoch 195</th>\n",
       "      <th>Epoch 196</th>\n",
       "      <th>Epoch 197</th>\n",
       "      <th>Epoch 198</th>\n",
       "      <th>Epoch 199</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308099</td>\n",
       "      <td>0.321216</td>\n",
       "      <td>0.334178</td>\n",
       "      <td>0.346927</td>\n",
       "      <td>0.359416</td>\n",
       "      <td>0.371605</td>\n",
       "      <td>0.383473</td>\n",
       "      <td>0.395011</td>\n",
       "      <td>0.406225</td>\n",
       "      <td>0.417120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522774</td>\n",
       "      <td>0.522399</td>\n",
       "      <td>0.522027</td>\n",
       "      <td>0.521658</td>\n",
       "      <td>0.521292</td>\n",
       "      <td>0.520928</td>\n",
       "      <td>0.520566</td>\n",
       "      <td>0.520206</td>\n",
       "      <td>0.519849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.312261</td>\n",
       "      <td>0.325062</td>\n",
       "      <td>0.337692</td>\n",
       "      <td>0.350099</td>\n",
       "      <td>0.362237</td>\n",
       "      <td>0.374070</td>\n",
       "      <td>0.385578</td>\n",
       "      <td>0.396756</td>\n",
       "      <td>0.407609</td>\n",
       "      <td>0.418145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494547</td>\n",
       "      <td>0.494071</td>\n",
       "      <td>0.493599</td>\n",
       "      <td>0.493131</td>\n",
       "      <td>0.492665</td>\n",
       "      <td>0.492202</td>\n",
       "      <td>0.491741</td>\n",
       "      <td>0.491284</td>\n",
       "      <td>0.490828</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.398290</td>\n",
       "      <td>0.403487</td>\n",
       "      <td>0.408636</td>\n",
       "      <td>0.413727</td>\n",
       "      <td>0.418751</td>\n",
       "      <td>0.423695</td>\n",
       "      <td>0.428549</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.437936</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176631</td>\n",
       "      <td>0.175708</td>\n",
       "      <td>0.174792</td>\n",
       "      <td>0.173884</td>\n",
       "      <td>0.172983</td>\n",
       "      <td>0.172089</td>\n",
       "      <td>0.171202</td>\n",
       "      <td>0.170322</td>\n",
       "      <td>0.169448</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397658</td>\n",
       "      <td>0.402910</td>\n",
       "      <td>0.408113</td>\n",
       "      <td>0.413257</td>\n",
       "      <td>0.418331</td>\n",
       "      <td>0.423324</td>\n",
       "      <td>0.428225</td>\n",
       "      <td>0.433020</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.442249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176647</td>\n",
       "      <td>0.175723</td>\n",
       "      <td>0.174808</td>\n",
       "      <td>0.173899</td>\n",
       "      <td>0.172998</td>\n",
       "      <td>0.172103</td>\n",
       "      <td>0.171216</td>\n",
       "      <td>0.170336</td>\n",
       "      <td>0.169463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.397767</td>\n",
       "      <td>0.403009</td>\n",
       "      <td>0.408203</td>\n",
       "      <td>0.413338</td>\n",
       "      <td>0.418403</td>\n",
       "      <td>0.423388</td>\n",
       "      <td>0.428280</td>\n",
       "      <td>0.433069</td>\n",
       "      <td>0.437740</td>\n",
       "      <td>0.442283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176644</td>\n",
       "      <td>0.175721</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.173896</td>\n",
       "      <td>0.172995</td>\n",
       "      <td>0.172101</td>\n",
       "      <td>0.171214</td>\n",
       "      <td>0.170333</td>\n",
       "      <td>0.169460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658328</th>\n",
       "      <td>0.200027</td>\n",
       "      <td>0.219931</td>\n",
       "      <td>0.240499</td>\n",
       "      <td>0.261562</td>\n",
       "      <td>0.282943</td>\n",
       "      <td>0.304471</td>\n",
       "      <td>0.325996</td>\n",
       "      <td>0.347398</td>\n",
       "      <td>0.368579</td>\n",
       "      <td>0.389460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957818</td>\n",
       "      <td>0.958189</td>\n",
       "      <td>0.958555</td>\n",
       "      <td>0.958917</td>\n",
       "      <td>0.959275</td>\n",
       "      <td>0.959629</td>\n",
       "      <td>0.959979</td>\n",
       "      <td>0.960325</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658329</th>\n",
       "      <td>0.398128</td>\n",
       "      <td>0.403338</td>\n",
       "      <td>0.408501</td>\n",
       "      <td>0.413606</td>\n",
       "      <td>0.418643</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>0.428465</td>\n",
       "      <td>0.433228</td>\n",
       "      <td>0.437875</td>\n",
       "      <td>0.442395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176635</td>\n",
       "      <td>0.175712</td>\n",
       "      <td>0.174796</td>\n",
       "      <td>0.173888</td>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.172092</td>\n",
       "      <td>0.171205</td>\n",
       "      <td>0.170325</td>\n",
       "      <td>0.169452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658330</th>\n",
       "      <td>0.198856</td>\n",
       "      <td>0.218794</td>\n",
       "      <td>0.239412</td>\n",
       "      <td>0.260540</td>\n",
       "      <td>0.281998</td>\n",
       "      <td>0.303616</td>\n",
       "      <td>0.325242</td>\n",
       "      <td>0.346752</td>\n",
       "      <td>0.368047</td>\n",
       "      <td>0.389047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959273</td>\n",
       "      <td>0.959637</td>\n",
       "      <td>0.959996</td>\n",
       "      <td>0.960351</td>\n",
       "      <td>0.960701</td>\n",
       "      <td>0.961048</td>\n",
       "      <td>0.961391</td>\n",
       "      <td>0.961730</td>\n",
       "      <td>0.962065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658331</th>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.402502</td>\n",
       "      <td>0.407743</td>\n",
       "      <td>0.412924</td>\n",
       "      <td>0.418034</td>\n",
       "      <td>0.423062</td>\n",
       "      <td>0.427995</td>\n",
       "      <td>0.432823</td>\n",
       "      <td>0.437532</td>\n",
       "      <td>0.442110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176658</td>\n",
       "      <td>0.175734</td>\n",
       "      <td>0.174818</td>\n",
       "      <td>0.173909</td>\n",
       "      <td>0.173008</td>\n",
       "      <td>0.172114</td>\n",
       "      <td>0.171226</td>\n",
       "      <td>0.170346</td>\n",
       "      <td>0.169473</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658332</th>\n",
       "      <td>0.369026</td>\n",
       "      <td>0.377244</td>\n",
       "      <td>0.385285</td>\n",
       "      <td>0.393127</td>\n",
       "      <td>0.400751</td>\n",
       "      <td>0.408137</td>\n",
       "      <td>0.415267</td>\n",
       "      <td>0.422125</td>\n",
       "      <td>0.428697</td>\n",
       "      <td>0.434970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>0.176774</td>\n",
       "      <td>0.175778</td>\n",
       "      <td>0.174791</td>\n",
       "      <td>0.173813</td>\n",
       "      <td>0.172843</td>\n",
       "      <td>0.171881</td>\n",
       "      <td>0.170928</td>\n",
       "      <td>0.169983</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>658333 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Epoch 0   Epoch 1   Epoch 2   Epoch 3   Epoch 4   Epoch 5   Epoch 6  \\\n",
       "0       0.308099  0.321216  0.334178  0.346927  0.359416  0.371605  0.383473   \n",
       "1       0.312261  0.325062  0.337692  0.350099  0.362237  0.374070  0.385578   \n",
       "2       0.398290  0.403487  0.408636  0.413727  0.418751  0.423695  0.428549   \n",
       "3       0.397658  0.402910  0.408113  0.413257  0.418331  0.423324  0.428225   \n",
       "4       0.397767  0.403009  0.408203  0.413338  0.418403  0.423388  0.428280   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "658328  0.200027  0.219931  0.240499  0.261562  0.282943  0.304471  0.325996   \n",
       "658329  0.398128  0.403338  0.408501  0.413606  0.418643  0.423600  0.428465   \n",
       "658330  0.198856  0.218794  0.239412  0.260540  0.281998  0.303616  0.325242   \n",
       "658331  0.396900  0.402502  0.407743  0.412924  0.418034  0.423062  0.427995   \n",
       "658332  0.369026  0.377244  0.385285  0.393127  0.400751  0.408137  0.415267   \n",
       "\n",
       "         Epoch 7   Epoch 8   Epoch 9  ...  Epoch 191  Epoch 192  Epoch 193  \\\n",
       "0       0.395011  0.406225  0.417120  ...   0.522774   0.522399   0.522027   \n",
       "1       0.396756  0.407609  0.418145  ...   0.494547   0.494071   0.493599   \n",
       "2       0.433300  0.437936  0.442446  ...   0.176631   0.175708   0.174792   \n",
       "3       0.433020  0.437700  0.442249  ...   0.176647   0.175723   0.174808   \n",
       "4       0.433069  0.437740  0.442283  ...   0.176644   0.175721   0.174805   \n",
       "...          ...       ...       ...  ...        ...        ...        ...   \n",
       "658328  0.347398  0.368579  0.389460  ...   0.957818   0.958189   0.958555   \n",
       "658329  0.433228  0.437875  0.442395  ...   0.176635   0.175712   0.174796   \n",
       "658330  0.346752  0.368047  0.389047  ...   0.959273   0.959637   0.959996   \n",
       "658331  0.432823  0.437532  0.442110  ...   0.176658   0.175734   0.174818   \n",
       "658332  0.422125  0.428697  0.434970  ...   0.177779   0.176774   0.175778   \n",
       "\n",
       "        Epoch 194  Epoch 195  Epoch 196  Epoch 197  Epoch 198  Epoch 199  \\\n",
       "0        0.521658   0.521292   0.520928   0.520566   0.520206   0.519849   \n",
       "1        0.493131   0.492665   0.492202   0.491741   0.491284   0.490828   \n",
       "2        0.173884   0.172983   0.172089   0.171202   0.170322   0.169448   \n",
       "3        0.173899   0.172998   0.172103   0.171216   0.170336   0.169463   \n",
       "4        0.173896   0.172995   0.172101   0.171214   0.170333   0.169460   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "658328   0.958917   0.959275   0.959629   0.959979   0.960325   0.960667   \n",
       "658329   0.173888   0.172987   0.172092   0.171205   0.170325   0.169452   \n",
       "658330   0.960351   0.960701   0.961048   0.961391   0.961730   0.962065   \n",
       "658331   0.173909   0.173008   0.172114   0.171226   0.170346   0.169473   \n",
       "658332   0.174791   0.173813   0.172843   0.171881   0.170928   0.169983   \n",
       "\n",
       "        Classification  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "...                ...  \n",
       "658328             1.0  \n",
       "658329             0.0  \n",
       "658330             1.0  \n",
       "658331             0.0  \n",
       "658332             0.0  \n",
       "\n",
       "[658333 rows x 201 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afadb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in signalnames:    \n",
    "   # signalname='GluGluToRadionToHHTo2G2Tau_M-1000'\n",
    "    signalname=thing\n",
    "    sig = df[df.process_id == proc_dict[f\"{signalname}\"]] # just one signal process, mass of X is 1000 GeV\n",
    "    sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n",
    "    \"\"\"Concatenating the background data\"\"\"\n",
    "    background_list=['Data','DiPhoton', 'TTGG', 'TTGamma',#list of each bkgs for concatenation\n",
    "     'TTJets',\n",
    "     'VBFH_M125',\n",
    "     'VH_M125',\n",
    "     'WGamma',\n",
    "     'ZGamma',\n",
    "     'ggH_M125', \n",
    "     'ttH_M125',\n",
    "     'GJets']\n",
    "\n",
    "    listforconc=[]\n",
    "    for i in background_list:                               #creating a concatenated list of bkg\n",
    "        bkgg = df[df.process_id == proc_dict[i]]\n",
    "        listforconc.append(bkgg)\n",
    "\n",
    "    background = pd.concat(listforconc)\n",
    "    background['Classification']=np.zeros(background['Diphoton_mass'].size)\n",
    "\n",
    "    \"\"\"The features requiring exclusion of -9 values\"\"\"\n",
    "    MinusNineBinning=['ditau_met_dPhi',\n",
    "                      'ditau_deta',\n",
    "                      'ditau_dR',\n",
    "                      'ditau_dphi',\n",
    "                      'ditau_pt',\n",
    "                      'Diphoton_ditau_dphi',\n",
    "                      'dilep_leadpho_mass',\n",
    "                      'reco_MX_mgg',\n",
    "                      'Diphoton_ditau_deta',\n",
    "                      'Diphoton_sublead_lepton_deta',\n",
    "                      'Diphoton_sublead_lepton_dR',\n",
    "                      'LeadPhoton_ditau_dR',\n",
    "                      'ditau_mass']\n",
    "\n",
    "    \"\"\"Concatenating Signal and Background\"\"\"\n",
    "    \"\"\"Choosing Best Features given the M=1000 AUC scores\"\"\"\n",
    "    FullSignalBackground=pd.concat([sig,background])\n",
    "\n",
    "    df_TopFeatures=pd.DataFrame()\n",
    "\n",
    "    #   TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "    # includes classification and weights\n",
    "    #TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','weight_central','Classification']\n",
    "    #    TopFeatures=['reco_MX_mgg','weight_central','Classification']\n",
    "\n",
    "    \"\"\"A dataset consisting of only the essential features\"\"\"\n",
    "    for feature in TopFeatures:\n",
    "        df_TopFeatures[feature]=FullSignalBackground[feature]\n",
    "\n",
    "    \"\"\"Removal of the values that are binned at -9 from the necessary features\"\"\"\n",
    "    for columns in df_TopFeatures.columns:\n",
    "        if columns in MinusNineBinning:\n",
    "            df_TopFeatures = df_TopFeatures.loc[(df_TopFeatures[columns] > -8)]\n",
    "\n",
    "    df_TopFeatures = df_TopFeatures.sample(frac=1, random_state=42)  # Setting frac=1 shuffles all rows\n",
    "\n",
    "    features = df_TopFeatures # Extracting features\n",
    "\n",
    "    labels = df_TopFeatures['Classification']  # Extracting labels\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=(1/3), random_state=42)\n",
    "    test_weights=pd.DataFrame()\n",
    "    train_weights=pd.DataFrame()\n",
    "\n",
    "    weightofsignal=train_features[train_features['Classification']==1]['weight_central'].sum()\n",
    "    weightofbackground=train_features[train_features['Classification']==0]['weight_central'].sum()\n",
    "    scale=weightofsignal/weightofbackground\n",
    "\n",
    "    \"\"\"reweighting the weight_central column in entire data \n",
    "    set such that for background and signal \"\"\"\n",
    "    train_features.loc[train_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "    test_features.loc[test_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "\n",
    "\n",
    "    train_weights['weight_central']=train_features['weight_central']\n",
    "    test_weights['weight_central']=test_features['weight_central']\n",
    "\n",
    "\n",
    "\n",
    "    train_features = train_features.drop(columns=['weight_central'])\n",
    "    train_features = train_features.drop(columns=['Classification'])\n",
    "    test_features=test_features.drop(columns=['weight_central'])\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "    train_weights_tensor = torch.tensor(train_weights.values,dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels.values,dtype=torch.float32)\n",
    "\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN,self).__init__()\n",
    "            self.hidden1 = nn.Linear(1, 8)\n",
    "            self.act1 = nn.ReLU()\n",
    "    #         self.hidden2 = nn.Linear(20, 8)\n",
    "    #         self.act2 = nn.ReLU()\n",
    "    #         self.output = nn.Linear(8, 1)\n",
    "    #         self.hidden1 = nn.Linear(16, 8)\n",
    "    #         self.act1 = nn.ReLU()\n",
    "    #         self.hidden2 = nn.Linear(20, 40)\n",
    "    #         self.act2 = nn.ReLU()\n",
    "    #         self.hidden3 = nn.Linear(40, 16)\n",
    "    #         self.act3 = nn.ReLU()\n",
    "    #         self.hidden4 = nn.Linear(16, 8)\n",
    "    #         self.act4 = nn.ReLU()\n",
    "            self.output = nn.Linear(8, 1)\n",
    "            self.act_output = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.hidden1(x)\n",
    "            x = self.act1(x)\n",
    "          #  x = self.hidden2(x)\n",
    "          #  x = self.act2(x)\n",
    "          #  x = self.hidden3(x)\n",
    "          #  x = self.act3(x)\n",
    "          #  x = self.hidden4(x)\n",
    "          #  x = self.act4(x)\n",
    "            x = self.output(x)\n",
    "            x = self.act_output(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "        def weightedBCELoss(self, input, target, weight):\n",
    "          x, y, w = input, target, weight\n",
    "          log = lambda x: torch.log(x*(1-1e-8) + 1e-8)\n",
    "          #return torch.mean(-w * (y*log(x) + (1-y)*log(1-x)))\n",
    "          return -w * (y*log(x) + (1-y)*log(1-x))\n",
    "\n",
    "        def batch_weightedBCELoss(self, input, target, weight, batch_size):\n",
    "    #batch_weightedBCELoss(self, train, train_labels_tensor, train_weights_tensor, batch_size)\n",
    "            self.batch_size=batch_size\n",
    "\n",
    "            target=target.unsqueeze(1)\n",
    "\n",
    "\n",
    "           # train=model.forward(input)\n",
    "\n",
    "            total_batch_err=torch.empty(0,1)\n",
    "            output_length=input.shape[0]\n",
    "            batch_remainder=output_length%batch_size\n",
    "\n",
    "            for i in range(0, output_length//batch_size):\n",
    "                weights = weight[i*(batch_size):(i+1)*(batch_size), :]\n",
    "                labels = target[i*(batch_size):(i+1)*(batch_size), :]\n",
    "                inputs = input[i*(batch_size):(i+1)*(batch_size), :]\n",
    "\n",
    "                loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "                total_batch_err=torch.cat((total_batch_err,loss)) \n",
    "            #    print(total_batch_err.shape[0])\n",
    "\n",
    "            if batch_remainder > 0:\n",
    "                weights = weight[(output_length//batch_size)*batch_size:, :]\n",
    "                labels = target[(output_length//batch_size)*batch_size:, :]\n",
    "                inputs = input[(output_length//batch_size)*batch_size:, :]\n",
    "\n",
    "                loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "                #weights = train_weights_tensor[(train_weights_tensor.shape[0]//batch_size)*batch_size:, :]\n",
    "                total_batch_err=torch.cat((total_batch_err,loss))\n",
    "            #    print(total_batch_err.shape[0])\n",
    "\n",
    "            return torch.mean(total_batch_err)\n",
    "\n",
    "\n",
    "    model = SimpleNN()\n",
    "    lr=0.01\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    lossdata=[]\n",
    "    df_Prediction=pd.DataFrame()\n",
    "    #    epochs=200\n",
    "    epochlist=[]\n",
    "    for i in range(1,epochs+1):\n",
    "        epochlist.append(i)\n",
    "#     for i in range(0,epochs):\n",
    "#         trained=model.forward(train_features_tensor)\n",
    "#         trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "#         df_Prediction[f'Epoch {i}'] = trained_data.copy()\n",
    "#         loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "#         lossdata.append(loss.item())\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "    trained_data_list = []\n",
    "    for i in range(0,epochs):\n",
    "        trained=model.forward(train_features_tensor)\n",
    "        trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "        trained_data_list.append(trained_data.copy())  # Append trained_data to the list\n",
    "\n",
    "    #    df_Prediction[f'Epoch {i}'] = trained_data\n",
    "        loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "        lossdata.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "    df_Prediction = pd.concat(trained_data_list, axis=1)\n",
    "    df_Prediction.columns = [f'Epoch {i}' for i in range(epochs)]\n",
    "\n",
    "\n",
    "\n",
    "    # figure=plt.figure()\n",
    "    # plt.plot(epochlist,lossdata)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.show()    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Plot of Loss vs Epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # figure=plt.figure()\n",
    "    # plt.plot(epochlist,lossdata)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.show()    \n",
    "\n",
    "    #epoch_= 'Epoch 99'\n",
    "    \"\"\"Adding the labels to the prediction dataframe\"\"\"\n",
    "    #epoch_= 'Epoch 99'\n",
    "    epoch_=f'Epoch {epochs-1}'\n",
    "    train_labels_=pd.DataFrame({'Classification': train_labels}).reset_index(drop=True)\n",
    "    df_Prediction = pd.concat([df_Prediction, train_labels_], axis=1)\n",
    "    epoch_=f'Epoch {epochs-1}'\n",
    "    # plt.figure()\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Classification of Events')\n",
    "    # plt.ylabel('Number of Events')\n",
    "    # plt.title('Comparison of the expected output and the trained output')\n",
    "    # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "    # plt.show()\n",
    "\n",
    "    df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "    fpr, tpr, thresholds = roc_curve(df_Prediction['Classification'], df_Prediction[epoch_])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig=plt.figure()    \n",
    "    plt.plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # #epoch_= 'Epoch 99'\n",
    "    # epoch_=f'Epoch {epochs-1}'\n",
    "    # plt.figure()\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Classification of Events')\n",
    "    # plt.ylabel('Number of Events')\n",
    "    # plt.title('Comparison of the expected output and the trained output')\n",
    "    # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "    # Plot 1: Line plot (plt.plot)\n",
    "    axs[1].plot(epochlist, lossdata)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_title('Loss per Epoch')\n",
    "\n",
    "    # Plot 2: Histograms (two separate subplots)\n",
    "\n",
    "    axs[0].set_title('Trained Histogram')\n",
    "\n",
    "    axs[0].set_xlabel('Trained Classification')\n",
    "    axs[0].set_ylabel('Number Events')\n",
    "\n",
    "    axs[0].hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\",range=(0,1))\n",
    "    axs[0].hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background',range=(0,1))\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title('Expected Histogram')\n",
    "\n",
    "    # Plot 3: Scatter plot with AUC score\n",
    "    # axs[2].scatter(FPR_arr, TPR_arr, label=f'AUC: {AUCscore}')\n",
    "    # axs[2].set_xlabel('FPR')\n",
    "    # axs[2].set_ylabel('TPR')\n",
    "    # axs[2].legend()\n",
    "    # axs[2].set_title('ROC Curve')\n",
    "\n",
    "    axs[2].plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "    axs[2].set_xlabel('FPR')\n",
    "    axs[2].set_ylabel('TPR')\n",
    "    axs[2].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "    axs[2].legend()\n",
    "\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_1\")\n",
    "    print(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_\")\n",
    "\n",
    "    plt.show()\n",
    "    print(roc_auc)\n",
    "    aucscoreslist.append(roc_auc)\n",
    "    name+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e13fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5343f3c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (18,) and (19,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m masses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m900\u001b[39m,\u001b[38;5;241m800\u001b[39m,\u001b[38;5;241m750\u001b[39m,\u001b[38;5;241m700\u001b[39m,\u001b[38;5;241m650\u001b[39m,\u001b[38;5;241m600\u001b[39m,\u001b[38;5;241m550\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m450\u001b[39m,\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m350\u001b[39m,\u001b[38;5;241m320\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m290\u001b[39m,\u001b[38;5;241m280\u001b[39m,\u001b[38;5;241m270\u001b[39m,\u001b[38;5;241m260\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maucscoreslist\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC Score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2769\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2770\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2771\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (18,) and (19,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masses=[1000,900,800,750,700,650,600,550,500,450,400,350,320,300,290,280,270,260]\n",
    "plt.plot(masses[0:name],aucscoreslist,'o')\n",
    "plt.xlabel('Mx')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title(f'{TopFeatures[0]}: Plot of AUC Score for each Mx at {epochs} epochs')\n",
    "plt.savefig(f\"AUC_v_Mx_{TopFeatures[0]}_epochs={epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c26a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopFeatures=['reco_MX_mgg','weight_central','Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f83d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(masses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d79a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
