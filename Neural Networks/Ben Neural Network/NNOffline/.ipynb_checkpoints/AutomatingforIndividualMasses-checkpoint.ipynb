{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a94fdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGenerating neural networks for individual masses \\nAutomated across each feature to allow direct comparison \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generating neural networks for individual masses \n",
    "Automated across each feature to allow direct comparison\n",
    "\n",
    "allows investigation into learning rate and architecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306149bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#%matplotlib nbagg\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "df = pd.read_parquet(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\merged_nominal.parquet')\n",
    "\n",
    "\n",
    "with open(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\summary.json', \"r\") as f:\n",
    "  proc_dict = json.load(f)[\"sample_id_map\"]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "signalnames = ['GluGluToRadionToHHTo2G2Tau_M-260',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-270',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-280',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-290',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-300',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-320',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-350',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-400',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-450',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-500',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-550',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-600',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-650',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-700',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-750',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-800',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-900',\n",
    " 'GluGluToRadionToHHTo2G2Tau_M-1000']\n",
    "signalnames.reverse()\n",
    "#TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "\n",
    "TopFeatures=['ditau_met_dPhi','weight_central','Classification']\n",
    "#TopFeatures=['lead_lepton_pt','weight_central','Classification']\n",
    "savefig='yes'\n",
    "epochs=200\n",
    "lr=0.03\n",
    "learningrate='0_03'\n",
    "aucscoreslist=[]\n",
    "name=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in signalnames:    \n",
    "    #signalname='GluGluToRadionToHHTo2G2Tau_M-700'\n",
    "    signalname=thing\n",
    "    sig = df[df.process_id == proc_dict[f\"{signalname}\"]] # just one signal process, mass of X is 1000 GeV\n",
    "    sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n",
    "    \"\"\"Concatenating the background data\"\"\"\n",
    "    background_list=['Data','DiPhoton', 'TTGG', 'TTGamma',#list of each bkgs for concatenation\n",
    "     'TTJets',\n",
    "     'VBFH_M125',\n",
    "     'VH_M125',\n",
    "     'WGamma',\n",
    "     'ZGamma',\n",
    "     'ggH_M125', \n",
    "     'ttH_M125',\n",
    "     'GJets']\n",
    "\n",
    "    listforconc=[]\n",
    "    for i in background_list:                               #creating a concatenated list of bkg\n",
    "        bkgg = df[df.process_id == proc_dict[i]]\n",
    "        listforconc.append(bkgg)\n",
    "\n",
    "    background = pd.concat(listforconc)\n",
    "    background['Classification']=np.zeros(background['Diphoton_mass'].size)\n",
    "\n",
    "    \"\"\"The features requiring exclusion of -9 values\"\"\"\n",
    "    MinusNineBinning=['ditau_met_dPhi',\n",
    "                      'ditau_deta',\n",
    "                      'ditau_dR',\n",
    "                      'ditau_dphi',\n",
    "                      'ditau_pt',\n",
    "                      'Diphoton_ditau_dphi',\n",
    "                      'dilep_leadpho_mass',\n",
    "                      'reco_MX_mgg',\n",
    "                      'Diphoton_ditau_deta',\n",
    "                      'Diphoton_sublead_lepton_deta',\n",
    "                      'Diphoton_sublead_lepton_dR',\n",
    "                      'LeadPhoton_ditau_dR',\n",
    "                      'ditau_mass']\n",
    "\n",
    "    \"\"\"Concatenating Signal and Background\"\"\"\n",
    "    \"\"\"Choosing Best Features given the M=1000 AUC scores\"\"\"\n",
    "    FullSignalBackground=pd.concat([sig,background])\n",
    "\n",
    "    df_TopFeatures=pd.DataFrame()\n",
    "\n",
    "    #   TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "    # includes classification and weights\n",
    "    #TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','weight_central','Classification']\n",
    "    #    TopFeatures=['reco_MX_mgg','weight_central','Classification']\n",
    "\n",
    "    \"\"\"A dataset consisting of only the essential features\"\"\"\n",
    "    for feature in TopFeatures:\n",
    "        df_TopFeatures[feature]=FullSignalBackground[feature]\n",
    "\n",
    "    \"\"\"Removal of the values that are binned at -9 from the necessary features\"\"\"\n",
    "    for columns in df_TopFeatures.columns:\n",
    "        if columns in MinusNineBinning:\n",
    "            df_TopFeatures = df_TopFeatures.loc[(df_TopFeatures[columns] > -8)]\n",
    "\n",
    "    df_TopFeatures = df_TopFeatures.sample(frac=1, random_state=42)  # Setting frac=1 shuffles all rows\n",
    "\n",
    "    features = df_TopFeatures # Extracting features\n",
    "\n",
    "    labels = df_TopFeatures['Classification']  # Extracting labels\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=(1/3), random_state=42)\n",
    "    test_weights=pd.DataFrame()\n",
    "    train_weights=pd.DataFrame()\n",
    "\n",
    "    weightofsignal=train_features[train_features['Classification']==1]['weight_central'].sum()\n",
    "    weightofbackground=train_features[train_features['Classification']==0]['weight_central'].sum()\n",
    "    scale=weightofsignal/weightofbackground\n",
    "\n",
    "    \"\"\"reweighting the weight_central column in entire data \n",
    "    set such that for background and signal \"\"\"\n",
    "    train_features.loc[train_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "    test_features.loc[test_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "\n",
    "\n",
    "    train_weights['weight_central']=train_features['weight_central']\n",
    "    test_weights['weight_central']=test_features['weight_central']\n",
    "\n",
    "\n",
    "\n",
    "    train_features = train_features.drop(columns=['weight_central'])\n",
    "    train_features = train_features.drop(columns=['Classification'])\n",
    "    test_features=test_features.drop(columns=['weight_central'])\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "    train_weights_tensor = torch.tensor(train_weights.values,dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels.values,dtype=torch.float32)\n",
    "\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN,self).__init__()\n",
    "            self.hidden1 = nn.Linear(1, 8)\n",
    "            self.act1 = nn.ReLU()\n",
    "    #         self.hidden2 = nn.Linear(20, 8)\n",
    "    #         self.act2 = nn.ReLU()\n",
    "    #         self.output = nn.Linear(8, 1)\n",
    "    #         self.hidden1 = nn.Linear(16, 8)\n",
    "    #         self.act1 = nn.ReLU()\n",
    "    #         self.hidden2 = nn.Linear(20, 40)\n",
    "    #         self.act2 = nn.ReLU()\n",
    "    #         self.hidden3 = nn.Linear(40, 16)\n",
    "    #         self.act3 = nn.ReLU()\n",
    "    #         self.hidden4 = nn.Linear(16, 8)\n",
    "    #         self.act4 = nn.ReLU()\n",
    "            self.output = nn.Linear(8, 1)\n",
    "            self.act_output = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.hidden1(x)\n",
    "            x = self.act1(x)\n",
    "          #  x = self.hidden2(x)\n",
    "          #  x = self.act2(x)\n",
    "          #  x = self.hidden3(x)\n",
    "          #  x = self.act3(x)\n",
    "          #  x = self.hidden4(x)\n",
    "          #  x = self.act4(x)\n",
    "            x = self.output(x)\n",
    "            x = self.act_output(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "        def weightedBCELoss(self, input, target, weight):\n",
    "          x, y, w = input, target, weight\n",
    "          log = lambda x: torch.log(x*(1-1e-8) + 1e-8)\n",
    "          #return torch.mean(-w * (y*log(x) + (1-y)*log(1-x)))\n",
    "          return -w * (y*log(x) + (1-y)*log(1-x))\n",
    "\n",
    "        def batch_weightedBCELoss(self, input, target, weight, batch_size):\n",
    "    #batch_weightedBCELoss(self, train, train_labels_tensor, train_weights_tensor, batch_size)\n",
    "            self.batch_size=batch_size\n",
    "\n",
    "            target=target.unsqueeze(1)\n",
    "\n",
    "\n",
    "           # train=model.forward(input)\n",
    "\n",
    "            total_batch_err=torch.empty(0,1)\n",
    "            output_length=input.shape[0]\n",
    "            batch_remainder=output_length%batch_size\n",
    "\n",
    "            for i in range(0, output_length//batch_size):\n",
    "                weights = weight[i*(batch_size):(i+1)*(batch_size), :]\n",
    "                labels = target[i*(batch_size):(i+1)*(batch_size), :]\n",
    "                inputs = input[i*(batch_size):(i+1)*(batch_size), :]\n",
    "\n",
    "                loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "                total_batch_err=torch.cat((total_batch_err,loss)) \n",
    "            #    print(total_batch_err.shape[0])\n",
    "\n",
    "            if batch_remainder > 0:\n",
    "                weights = weight[(output_length//batch_size)*batch_size:, :]\n",
    "                labels = target[(output_length//batch_size)*batch_size:, :]\n",
    "                inputs = input[(output_length//batch_size)*batch_size:, :]\n",
    "\n",
    "                loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "                #weights = train_weights_tensor[(train_weights_tensor.shape[0]//batch_size)*batch_size:, :]\n",
    "                total_batch_err=torch.cat((total_batch_err,loss))\n",
    "            #    print(total_batch_err.shape[0])\n",
    "\n",
    "            return torch.mean(total_batch_err)\n",
    "\n",
    "\n",
    "    model = SimpleNN()\n",
    "    #lr=0.01\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "    lossdata=[]\n",
    "    df_Prediction=pd.DataFrame()\n",
    "    #    epochs=200\n",
    "    epochlist=[]\n",
    "    for i in range(1,epochs+1):\n",
    "        epochlist.append(i)\n",
    "#     for i in range(0,epochs):\n",
    "#         trained=model.forward(train_features_tensor)\n",
    "#         trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "#         df_Prediction[f'Epoch {i}'] = trained_data.copy()\n",
    "#         loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "#         lossdata.append(loss.item())\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "    trained_data_list = []\n",
    "    for i in range(0,epochs):\n",
    "        trained=model.forward(train_features_tensor)\n",
    "        trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "        trained_data_list.append(trained_data.copy())  # Append trained_data to the list\n",
    "\n",
    "    #    df_Prediction[f'Epoch {i}'] = trained_data\n",
    "        loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "        lossdata.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "    df_Prediction = pd.concat(trained_data_list, axis=1)\n",
    "    df_Prediction.columns = [f'Epoch {i}' for i in range(epochs)]\n",
    "\n",
    "\n",
    "\n",
    "    # figure=plt.figure()\n",
    "    # plt.plot(epochlist,lossdata)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.show()    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Plot of Loss vs Epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # figure=plt.figure()\n",
    "    # plt.plot(epochlist,lossdata)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.show()    \n",
    "\n",
    "    #epoch_= 'Epoch 99'\n",
    "    \"\"\"Adding the labels to the prediction dataframe\"\"\"\n",
    "    #epoch_= 'Epoch 99'\n",
    "    epoch_=f'Epoch {epochs-1}'\n",
    "    train_labels_=pd.DataFrame({'Classification': train_labels}).reset_index(drop=True)\n",
    "    df_Prediction = pd.concat([df_Prediction, train_labels_], axis=1)\n",
    "    epoch_=f'Epoch {epochs-1}'\n",
    "    # plt.figure()\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Classification of Events')\n",
    "    # plt.ylabel('Number of Events')\n",
    "    # plt.title('Comparison of the expected output and the trained output')\n",
    "    # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "    # plt.show()\n",
    "\n",
    "    df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "    fpr, tpr, thresholds = roc_curve(df_Prediction['Classification'], df_Prediction[epoch_])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig=plt.figure()    \n",
    "    plt.plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # #epoch_= 'Epoch 99'\n",
    "    # epoch_=f'Epoch {epochs-1}'\n",
    "    # plt.figure()\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "    # plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Classification of Events')\n",
    "    # plt.ylabel('Number of Events')\n",
    "    # plt.title('Comparison of the expected output and the trained output')\n",
    "    # #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "    # Plot 1: Line plot (plt.plot)\n",
    "    axs[1].plot(epochlist, lossdata)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_title(f'Loss per Epoch: {TopFeatures[0]}',fontsize=18)\n",
    "\n",
    "    # Plot 2: Histograms (two separate subplots)\n",
    "\n",
    "    axs[0].set_title('Trained Histogram',fontsize=18)\n",
    "\n",
    "    axs[0].set_xlabel('Trained Classification')\n",
    "    axs[0].set_ylabel('Number Events')\n",
    "\n",
    "    axs[0].hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\",range=(0,1))\n",
    "    axs[0].hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background',range=(0,1))\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title('Expected Histogram',fontsize=18)\n",
    "\n",
    "    # Plot 3: Scatter plot with AUC score\n",
    "    # axs[2].scatter(FPR_arr, TPR_arr, label=f'AUC: {AUCscore}')\n",
    "    # axs[2].set_xlabel('FPR')\n",
    "    # axs[2].set_ylabel('TPR')\n",
    "    # axs[2].legend()\n",
    "    # axs[2].set_title('ROC Curve')\n",
    "\n",
    "    axs[2].plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "    axs[2].set_xlabel('FPR')\n",
    "    axs[2].set_ylabel('TPR')\n",
    "    axs[2].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "    axs[2].legend()\n",
    "\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_1\")\n",
    "    if savefig == 'yes':\n",
    "        plt.savefig(f\"Figures{TopFeatures[0]}/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_lr={learningrate}\")\n",
    "    print(f\"Figures{TopFeatures[0]}/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}_lr={learningrate}\")\n",
    "\n",
    "    plt.show()\n",
    "    print(roc_auc)\n",
    "    aucscoreslist.append(roc_auc)\n",
    "    name+=1\n",
    "df_AucScores = pd.DataFrame({'Column': aucscoreslist})\n",
    "# File path to write CSV\n",
    "file_path = f'AucScores_{TopFeatures[0]}.csv'\n",
    "df_AucScores.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
