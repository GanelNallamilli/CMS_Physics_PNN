{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c0ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Built after the CleanNeuralNetwork python file so to ensure no irreversible damage occurs\n",
    "\n",
    "\n",
    "FIRST BOX IS THE ENTIRE SCRIPT\n",
    "After is decomposed into fewer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#%matplotlib nbagg\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "\n",
    "df = pd.read_parquet(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\merged_nominal.parquet')\n",
    "\n",
    "with open(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\summary.json', \"r\") as f:\n",
    "  proc_dict = json.load(f)[\"sample_id_map\"]\n",
    "  \n",
    "signalname=\"GluGluToRadionToHHTo2G2Tau_M-900\"\n",
    "sig = df[df.process_id == proc_dict[f\"{signalname}\"]] # just one signal process, mass of X is 1000 GeV\n",
    "sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n",
    "\"\"\"Concatenating the background data\"\"\"\n",
    "background_list=['Data','DiPhoton', 'TTGG', 'TTGamma',#list of each bkgs for concatenation\n",
    " 'TTJets',\n",
    " 'VBFH_M125',\n",
    " 'VH_M125',\n",
    " 'WGamma',\n",
    " 'ZGamma',\n",
    " 'ggH_M125', \n",
    " 'ttH_M125',\n",
    " 'GJets']\n",
    "\n",
    "listforconc=[]\n",
    "for i in background_list:                               #creating a concatenated list of bkg\n",
    "    bkgg = df[df.process_id == proc_dict[i]]\n",
    "    listforconc.append(bkgg)\n",
    "    \n",
    "background = pd.concat(listforconc)\n",
    "background['Classification']=np.zeros(background['Diphoton_mass'].size)\n",
    "\n",
    "\"\"\"The features requiring exclusion of -9 values\"\"\"\n",
    "MinusNineBinning=['ditau_met_dPhi',\n",
    "                  'ditau_deta',\n",
    "                  'ditau_dR',\n",
    "                  'ditau_dphi',\n",
    "                  'ditau_pt',\n",
    "                  'Diphoton_ditau_dphi',\n",
    "                  'dilep_leadpho_mass',\n",
    "                  'reco_MX_mgg',\n",
    "                  'Diphoton_ditau_deta',\n",
    "                  'Diphoton_sublead_lepton_deta',\n",
    "                  'Diphoton_sublead_lepton_dR',\n",
    "                  'LeadPhoton_ditau_dR',\n",
    "                  'ditau_mass']\n",
    "\n",
    "\"\"\"Concatenating Signal and Background\"\"\"\n",
    "\"\"\"Choosing Best Features given the M=1000 AUC scores\"\"\"\n",
    "FullSignalBackground=pd.concat([sig,background])\n",
    "\n",
    "df_TopFeatures=pd.DataFrame()\n",
    "\n",
    "TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "# includes classification and weights\n",
    "#TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','weight_central','Classification']\n",
    "TopFeatures=['reco_MX_mgg','weight_central','Classification']\n",
    "\n",
    "\"\"\"A dataset consisting of only the essential features\"\"\"\n",
    "for feature in TopFeatures:\n",
    "    df_TopFeatures[feature]=FullSignalBackground[feature]\n",
    "    \n",
    "\"\"\"Removal of the values that are binned at -9 from the necessary features\"\"\"\n",
    "for columns in df_TopFeatures.columns:\n",
    "    if columns in MinusNineBinning:\n",
    "        df_TopFeatures = df_TopFeatures.loc[(df_TopFeatures[columns] > -8)]\n",
    "\n",
    "df_TopFeatures = df_TopFeatures.sample(frac=1, random_state=42)  # Setting frac=1 shuffles all rows\n",
    "\n",
    "features = df_TopFeatures # Extracting features\n",
    "\n",
    "labels = df_TopFeatures['Classification']  # Extracting labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=(1/3), random_state=42)\n",
    "test_weights=pd.DataFrame()\n",
    "train_weights=pd.DataFrame()\n",
    "\n",
    "weightofsignal=train_features[train_features['Classification']==1]['weight_central'].sum()\n",
    "weightofbackground=train_features[train_features['Classification']==0]['weight_central'].sum()\n",
    "scale=weightofsignal/weightofbackground\n",
    "\n",
    "\"\"\"reweighting the weight_central column in entire data \n",
    "set such that for background and signal \"\"\"\n",
    "train_features.loc[train_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "test_features.loc[test_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "\n",
    "\n",
    "train_weights['weight_central']=train_features['weight_central']\n",
    "test_weights['weight_central']=test_features['weight_central']\n",
    "\n",
    "\n",
    "\n",
    "train_features = train_features.drop(columns=['weight_central'])\n",
    "train_features = train_features.drop(columns=['Classification'])\n",
    "test_features=test_features.drop(columns=['weight_central'])\n",
    "\n",
    "train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "train_weights_tensor = torch.tensor(train_weights.values,dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values,dtype=torch.float32)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN,self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, 8)\n",
    "        self.act1 = nn.ReLU()\n",
    "#         self.hidden2 = nn.Linear(20, 8)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.output = nn.Linear(8, 1)\n",
    "#         self.hidden1 = nn.Linear(16, 8)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.hidden2 = nn.Linear(20, 40)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.hidden3 = nn.Linear(40, 16)\n",
    "#         self.act3 = nn.ReLU()\n",
    "#         self.hidden4 = nn.Linear(16, 8)\n",
    "#         self.act4 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.act1(x)\n",
    "      #  x = self.hidden2(x)\n",
    "      #  x = self.act2(x)\n",
    "      #  x = self.hidden3(x)\n",
    "      #  x = self.act3(x)\n",
    "      #  x = self.hidden4(x)\n",
    "      #  x = self.act4(x)\n",
    "        x = self.output(x)\n",
    "        x = self.act_output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def weightedBCELoss(self, input, target, weight):\n",
    "      x, y, w = input, target, weight\n",
    "      log = lambda x: torch.log(x*(1-1e-8) + 1e-8)\n",
    "      #return torch.mean(-w * (y*log(x) + (1-y)*log(1-x)))\n",
    "      return -w * (y*log(x) + (1-y)*log(1-x))\n",
    "    \n",
    "    def batch_weightedBCELoss(self, input, target, weight, batch_size):\n",
    "#batch_weightedBCELoss(self, train, train_labels_tensor, train_weights_tensor, batch_size)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        target=target.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "       # train=model.forward(input)\n",
    "\n",
    "        total_batch_err=torch.empty(0,1)\n",
    "        output_length=input.shape[0]\n",
    "        batch_remainder=output_length%batch_size\n",
    "\n",
    "        for i in range(0, output_length//batch_size):\n",
    "            weights = weight[i*(batch_size):(i+1)*(batch_size), :]\n",
    "            labels = target[i*(batch_size):(i+1)*(batch_size), :]\n",
    "            inputs = input[i*(batch_size):(i+1)*(batch_size), :]\n",
    "\n",
    "            loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "            total_batch_err=torch.cat((total_batch_err,loss)) \n",
    "        #    print(total_batch_err.shape[0])\n",
    "\n",
    "        if batch_remainder > 0:\n",
    "            weights = weight[(output_length//batch_size)*batch_size:, :]\n",
    "            labels = target[(output_length//batch_size)*batch_size:, :]\n",
    "            inputs = input[(output_length//batch_size)*batch_size:, :]\n",
    "\n",
    "            loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "            #weights = train_weights_tensor[(train_weights_tensor.shape[0]//batch_size)*batch_size:, :]\n",
    "            total_batch_err=torch.cat((total_batch_err,loss))\n",
    "        #    print(total_batch_err.shape[0])\n",
    "            \n",
    "        return torch.mean(total_batch_err)\n",
    "\n",
    "\n",
    "model = SimpleNN()\n",
    "lr=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "lossdata=[]\n",
    "df_Prediction=pd.DataFrame()\n",
    "epochs=200\n",
    "epochlist=[]\n",
    "for i in range(1,epochs+1):\n",
    "    epochlist.append(i)\n",
    "for i in range(0,epochs):\n",
    "    trained=model.forward(train_features_tensor)\n",
    "    trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "    df_Prediction[f'Epoch {i}'] = trained_data\n",
    "    loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "    lossdata.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# figure=plt.figure()\n",
    "# plt.plot(epochlist,lossdata)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Plot of Loss vs Epoch\n",
    "\"\"\"\n",
    "\n",
    "# figure=plt.figure()\n",
    "# plt.plot(epochlist,lossdata)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()    \n",
    "\n",
    "#epoch_= 'Epoch 99'\n",
    "\"\"\"Adding the labels to the prediction dataframe\"\"\"\n",
    "#epoch_= 'Epoch 99'\n",
    "epoch_=f'Epoch {epochs-1}'\n",
    "train_labels_=pd.DataFrame({'Classification': train_labels}).reset_index(drop=True)\n",
    "df_Prediction = pd.concat([df_Prediction, train_labels_], axis=1)\n",
    "epoch_=f'Epoch {epochs-1}'\n",
    "# plt.figure()\n",
    "# plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "# plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Classification of Events')\n",
    "# plt.ylabel('Number of Events')\n",
    "# plt.title('Comparison of the expected output and the trained output')\n",
    "# #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "# plt.show()\n",
    "\n",
    "df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "fpr, tpr, thresholds = roc_curve(df_Prediction['Classification'], df_Prediction[epoch_])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fig=plt.figure()    \n",
    "plt.plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #epoch_= 'Epoch 99'\n",
    "# epoch_=f'Epoch {epochs-1}'\n",
    "# plt.figure()\n",
    "# plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "# plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Classification of Events')\n",
    "# plt.ylabel('Number of Events')\n",
    "# plt.title('Comparison of the expected output and the trained output')\n",
    "# #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "# Plot 1: Line plot (plt.plot)\n",
    "axs[1].plot(epochlist, lossdata)\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_title('Loss per Epoch')\n",
    "\n",
    "# Plot 2: Histograms (two separate subplots)\n",
    "\n",
    "axs[0].set_title('Trained Histogram')\n",
    "\n",
    "axs[0].set_xlabel('Trained Classification')\n",
    "axs[0].set_ylabel('Number Events')\n",
    "\n",
    "axs[0].hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "axs[0].hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Expected Histogram')\n",
    "\n",
    "# Plot 3: Scatter plot with AUC score\n",
    "# axs[2].scatter(FPR_arr, TPR_arr, label=f'AUC: {AUCscore}')\n",
    "# axs[2].set_xlabel('FPR')\n",
    "# axs[2].set_ylabel('TPR')\n",
    "# axs[2].legend()\n",
    "# axs[2].set_title('ROC Curve')\n",
    "  \n",
    "axs[2].plot(fpr,tpr,label=f'AUC:{AUCscore}')\n",
    "axs[2].set_xlabel('FPR')\n",
    "axs[2].set_ylabel('TPR')\n",
    "axs[2].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "axs[2].legend()\n",
    "\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}1\")\n",
    "print(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}\")\n",
    "\n",
    "plt.show()\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b774bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#%matplotlib nbagg\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "\n",
    "df = pd.read_parquet(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\merged_nominal.parquet')\n",
    "\n",
    "with open(r'C:\\Users\\drpla\\Desktop\\ICL-PHYSICS-YEAR-4\\Masters Project\\Data\\New folder\\summary.json', \"r\") as f:\n",
    "  proc_dict = json.load(f)[\"sample_id_map\"]\n",
    "  \n",
    "signalname=\"GluGluToRadionToHHTo2G2Tau_M-260\"\n",
    "sig = df[df.process_id == proc_dict[f\"{signalname}\"]] # just one signal process, mass of X is 1000 GeV\n",
    "sig['Classification']=np.ones(sig['Diphoton_mass'].size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b40823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Concatenating the background data\"\"\"\n",
    "background_list=['Data','DiPhoton', 'TTGG', 'TTGamma',#list of each bkgs for concatenation\n",
    " 'TTJets',\n",
    " 'VBFH_M125',\n",
    " 'VH_M125',\n",
    " 'WGamma',\n",
    " 'ZGamma',\n",
    " 'ggH_M125', \n",
    " 'ttH_M125',\n",
    " 'GJets']\n",
    "\n",
    "listforconc=[]\n",
    "for i in background_list:                               #creating a concatenated list of bkg\n",
    "    bkgg = df[df.process_id == proc_dict[i]]\n",
    "    listforconc.append(bkgg)\n",
    "    \n",
    "background = pd.concat(listforconc)\n",
    "background['Classification']=np.zeros(background['Diphoton_mass'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The features requiring exclusion of -9 values\"\"\"\n",
    "MinusNineBinning=['ditau_met_dPhi',\n",
    "                  'ditau_deta',\n",
    "                  'ditau_dR',\n",
    "                  'ditau_dphi',\n",
    "                  'ditau_pt',\n",
    "                  'Diphoton_ditau_dphi',\n",
    "                  'dilep_leadpho_mass',\n",
    "                  'reco_MX_mgg',\n",
    "                  'Diphoton_ditau_deta',\n",
    "                  'Diphoton_sublead_lepton_deta',\n",
    "                  'Diphoton_sublead_lepton_dR',\n",
    "                  'LeadPhoton_ditau_dR',\n",
    "                  'ditau_mass']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Concatenating Signal and Background\"\"\"\n",
    "\"\"\"Choosing Best Features given the M=1000 AUC scores\"\"\"\n",
    "FullSignalBackground=pd.concat([sig,background])\n",
    "\n",
    "df_TopFeatures=pd.DataFrame()\n",
    "\n",
    "TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','Diphoton_lead_lepton_deta','ditau_met_dPhi','ditau_deta','Diphoton_sublead_lepton_deta','Diphoton_ditau_deta','ditau_mass','weight_central','Classification']\n",
    "# includes classification and weights\n",
    "#TopFeatures=['reco_MX_mgg','Diphoton_pt_mgg','LeadPhoton_pt_mgg','ditau_pt','Diphoton_dPhi','dilep_leadpho_mass','lead_lepton_pt','MET_pt','ditau_dR','SubleadPhoton_pt_mgg','weight_central','Classification']\n",
    "TopFeatures=['reco_MX_mgg','weight_central','Classification']\n",
    "\n",
    "\"\"\"A dataset consisting of only the essential features\"\"\"\n",
    "for feature in TopFeatures:\n",
    "    df_TopFeatures[feature]=FullSignalBackground[feature]\n",
    "    \n",
    "\"\"\"Removal of the values that are binned at -9 from the necessary features\"\"\"\n",
    "for columns in df_TopFeatures.columns:\n",
    "    if columns in MinusNineBinning:\n",
    "        df_TopFeatures = df_TopFeatures.loc[(df_TopFeatures[columns] > -8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TopFeatures = df_TopFeatures.sample(frac=1, random_state=42)  # Setting frac=1 shuffles all rows\n",
    "\n",
    "features = df_TopFeatures # Extracting features\n",
    "\n",
    "labels = df_TopFeatures['Classification']  # Extracting labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=(1/3), random_state=42)\n",
    "test_weights=pd.DataFrame()\n",
    "train_weights=pd.DataFrame()\n",
    "\n",
    "weightofsignal=train_features[train_features['Classification']==1]['weight_central'].sum()\n",
    "weightofbackground=train_features[train_features['Classification']==0]['weight_central'].sum()\n",
    "scale=weightofsignal/weightofbackground\n",
    "\n",
    "\"\"\"reweighting the weight_central column in entire data \n",
    "set such that for background and signal \"\"\"\n",
    "train_features.loc[train_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "test_features.loc[test_features['Classification'] == 0, 'weight_central'] *= scale\n",
    "\n",
    "\n",
    "train_weights['weight_central']=train_features['weight_central']\n",
    "test_weights['weight_central']=test_features['weight_central']\n",
    "\n",
    "\n",
    "\n",
    "train_features = train_features.drop(columns=['weight_central'])\n",
    "train_features = train_features.drop(columns=['Classification'])\n",
    "test_features=test_features.drop(columns=['weight_central'])\n",
    "\n",
    "train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "train_weights_tensor = torch.tensor(train_weights.values,dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values,dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN,self).__init__()\n",
    "        self.hidden1 = nn.Linear(1, 8)\n",
    "        self.act1 = nn.ReLU()\n",
    "#         self.hidden2 = nn.Linear(20, 8)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.output = nn.Linear(8, 1)\n",
    "#         self.hidden1 = nn.Linear(16, 8)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.hidden2 = nn.Linear(20, 40)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.hidden3 = nn.Linear(40, 16)\n",
    "#         self.act3 = nn.ReLU()\n",
    "#         self.hidden4 = nn.Linear(16, 8)\n",
    "#         self.act4 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.act1(x)\n",
    "      #  x = self.hidden2(x)\n",
    "      #  x = self.act2(x)\n",
    "      #  x = self.hidden3(x)\n",
    "      #  x = self.act3(x)\n",
    "      #  x = self.hidden4(x)\n",
    "      #  x = self.act4(x)\n",
    "        x = self.output(x)\n",
    "        x = self.act_output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def weightedBCELoss(self, input, target, weight):\n",
    "      x, y, w = input, target, weight\n",
    "      log = lambda x: torch.log(x*(1-1e-8) + 1e-8)\n",
    "      #return torch.mean(-w * (y*log(x) + (1-y)*log(1-x)))\n",
    "      return -w * (y*log(x) + (1-y)*log(1-x))\n",
    "    \n",
    "    def batch_weightedBCELoss(self, input, target, weight, batch_size):\n",
    "#batch_weightedBCELoss(self, train, train_labels_tensor, train_weights_tensor, batch_size)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        target=target.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "       # train=model.forward(input)\n",
    "\n",
    "        total_batch_err=torch.empty(0,1)\n",
    "        output_length=input.shape[0]\n",
    "        batch_remainder=output_length%batch_size\n",
    "\n",
    "        for i in range(0, output_length//batch_size):\n",
    "            weights = weight[i*(batch_size):(i+1)*(batch_size), :]\n",
    "            labels = target[i*(batch_size):(i+1)*(batch_size), :]\n",
    "            inputs = input[i*(batch_size):(i+1)*(batch_size), :]\n",
    "\n",
    "            loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "            total_batch_err=torch.cat((total_batch_err,loss)) \n",
    "        #    print(total_batch_err.shape[0])\n",
    "\n",
    "        if batch_remainder > 0:\n",
    "            weights = weight[(output_length//batch_size)*batch_size:, :]\n",
    "            labels = target[(output_length//batch_size)*batch_size:, :]\n",
    "            inputs = input[(output_length//batch_size)*batch_size:, :]\n",
    "\n",
    "            loss=self.weightedBCELoss(inputs, labels, weights)\n",
    "\n",
    "            #weights = train_weights_tensor[(train_weights_tensor.shape[0]//batch_size)*batch_size:, :]\n",
    "            total_batch_err=torch.cat((total_batch_err,loss))\n",
    "        #    print(total_batch_err.shape[0])\n",
    "            \n",
    "        return torch.mean(total_batch_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd4c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "lr=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "lossdata=[]\n",
    "df_Prediction=pd.DataFrame()\n",
    "epochs=200\n",
    "epochlist=[]\n",
    "for i in range(1,epochs+1):\n",
    "    epochlist.append(i)\n",
    "for i in range(0,epochs):\n",
    "    trained=model.forward(train_features_tensor)\n",
    "    trained_data= pd.DataFrame(trained.detach().numpy())\n",
    "    df_Prediction[f'Epoch {i}'] = trained_data\n",
    "    loss=model.batch_weightedBCELoss(trained,train_labels_tensor,train_weights_tensor,1024)\n",
    "    lossdata.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'For Epoch {i+1}: Loss = {loss}')\n",
    "# figure=plt.figure()\n",
    "# plt.plot(epochlist,lossdata)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot of Loss vs Epoch\n",
    "\"\"\"\n",
    "\n",
    "figure=plt.figure()\n",
    "plt.plot(epochlist,lossdata)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb10bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding the labels to the prediction dataframe\"\"\"\n",
    "#epoch_= 'Epoch 99'\n",
    "epoch_=f'Epoch {epochs-1}'\n",
    "train_labels_=pd.DataFrame({'Classification': train_labels}).reset_index(drop=True)\n",
    "df_Prediction = pd.concat([df_Prediction, train_labels_], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f26627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #epoch_= 'Epoch 99'\n",
    "# epoch_=f'Epoch {epochs-1}'\n",
    "# plt.figure()\n",
    "# plt.hist(df_Prediction[epoch_],bins=80,label='trained',histtype=\"step\",color='orange')\n",
    "# plt.hist(train_labels,bins=80,histtype='step',color='grey',label='expected')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Classification of Events')\n",
    "# plt.ylabel('Number of Events')\n",
    "# plt.title('Comparison of the expected output and the trained output')\n",
    "# #plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba259d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_= 'Epoch 99'\n",
    "epoch_=f'Epoch {epochs-1}'\n",
    "plt.figure()\n",
    "plt.hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "plt.hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "plt.legend()\n",
    "plt.xlabel('Classification of Events')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Comparison of the expected output and the trained output')\n",
    "#plt.savefig(f\"BenNeuralNetworkPlots/TrainingHist-{signalname}Epochs={epochs}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "# import scipy as scipy\n",
    "\n",
    "# #%%\n",
    "# direction='above' #use this to determine the side of the threshold that calculates TPR and FPR\n",
    "\n",
    "# threshold_direction = f'Signal is {direction} Background'\n",
    "\n",
    "\n",
    "# minimum_edge=0\n",
    "# maximum_edge=1\n",
    "# steps=1000\n",
    "# stepsize=(maximum_edge-minimum_edge)/steps\n",
    "\n",
    "# threshold=minimum_edge\n",
    "# TPR_arr = []\n",
    "# FPR_arr = []\n",
    "# for i in range(0,steps): \n",
    "#     if threshold_direction == 'Signal is above Background':\n",
    "#         threshold += stepsize\n",
    "       \n",
    "#         TP = len(df_Prediction[(df_Prediction[epoch_] >= threshold) & (df_Prediction['Classification']==1)][epoch_])#\n",
    "#         FP = len(df_Prediction[(df_Prediction[epoch_] >= threshold) & (df_Prediction['Classification']==0)][epoch_])#\n",
    "#         FN = len(df_Prediction[(df_Prediction[epoch_] < threshold) & (df_Prediction['Classification']==1)][epoch_])#\n",
    "#         TN = len(df_Prediction[(df_Prediction[epoch_] < threshold) & (df_Prediction['Classification']==0)][epoch_])#\n",
    "#         print('run',i)\n",
    "                \n",
    "#         if (TP+FN) == 0:\n",
    "#             TPR = 0\n",
    "#             FPR = 0\n",
    "#         else:\n",
    "#             TPR = TP/(TP +FN)\n",
    "#             FPR = FP/(FP +TN)\n",
    "#         TPR_arr.append(TPR)\n",
    "#         FPR_arr.append(FPR)\n",
    "        \n",
    "#     elif threshold_direction == 'Signal is below Background':\n",
    "#         threshold += stepsize\n",
    "       \n",
    "#      #   TP = len(df_Prediction[(df_Prediction[epoch_] <= threshold) & (df_Prediction['Classification']==1)][epoch_])#\n",
    "#       #  FP = len(df_Prediction[(df_Prediction[epoch_] <= threshold) & (df_Prediction['Classification']==0)][epoch_])#\n",
    "#        # FN = len(df_Prediction[(df_Prediction[epoch_] > threshold) & (df_Prediction['Classification']==1)][epoch_])#\n",
    "#         #TN = len(df_Prediction[(df_Prediction[epoch_] > threshold) & (df_Prediction['Classification']==0)][epoch_])#\n",
    "        \n",
    "#         print('run',i)\n",
    "        \n",
    "#         TPR = TP/(TP +FN)\n",
    "#         FPR = FP/(FP +TN)\n",
    "#         TPR_arr.append(TPR)\n",
    "#         FPR_arr.append(FPR)\n",
    "        \n",
    "\n",
    "\n",
    "# sorted_values = sorted(zip(FPR_arr, TPR_arr))\n",
    "# FPR_arr, TPR_arr = zip(*sorted_values)\n",
    "\n",
    "df_Prediction.sort_values(by=[epoch_,'Classification'], ascending=True)\n",
    "fpr, tpr, thresholds = roc_curve(df_Prediction['Classification'], df_Prediction[epoch_])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fig=plt.figure()    \n",
    "plt.plot(fpr,tpr,label=f'AUC:{roc_auc}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8ffc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AUCscore=scipy.integrate.trapezoid(TPR_arr,FPR_arr,  dx=stepsize,axis=-1)\n",
    "# fig=plt.figure()    \n",
    "# plt.scatter(FPR_arr,TPR_arr,label=f'AUC:{AUCscore}')\n",
    "# plt.xlabel('FPR')\n",
    "# plt.ylabel('TPR')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# print(AUCscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Prediction = df_Prediction.drop(columns='Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "# Plot 1: Line plot (plt.plot)\n",
    "axs[1].plot(epochlist, lossdata)\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_title('Loss per Epoch')\n",
    "\n",
    "# Plot 2: Histograms (two separate subplots)\n",
    "\n",
    "axs[0].set_title('Trained Histogram')\n",
    "\n",
    "axs[0].set_xlabel('Trained Classification')\n",
    "axs[0].set_ylabel('Number Events')\n",
    "\n",
    "axs[0].hist(df_Prediction[df_Prediction['Classification']==1][epoch_],bins=80,label='predicted signal',histtype=\"step\")\n",
    "axs[0].hist(df_Prediction[df_Prediction['Classification']==0][epoch_],bins=80,histtype='step',label='predicted background')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Expected Histogram')\n",
    "\n",
    "# Plot 3: Scatter plot with AUC score\n",
    "# axs[2].scatter(FPR_arr, TPR_arr, label=f'AUC: {AUCscore}')\n",
    "# axs[2].set_xlabel('FPR')\n",
    "# axs[2].set_ylabel('TPR')\n",
    "# axs[2].legend()\n",
    "# axs[2].set_title('ROC Curve')\n",
    "  \n",
    "axs[2].plot(fpr,tpr,label=f'AUC:{AUCscore}')\n",
    "axs[2].set_xlabel('FPR')\n",
    "axs[2].set_ylabel('TPR')\n",
    "axs[2].plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guessing')\n",
    "axs[2].legend()\n",
    "\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}1\")\n",
    "print(f\"BenNeuralNetworkPlots/reco_MX_mgg_SimArch/HIST_LOSS_ROC_{signalname}=_Feature={TopFeatures[0]}_Epochs={epochs}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ca333",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e749c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_Prediction[epoch_].shape,train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f882d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(sig[sig['reco_MX_mgg']>-5],bins=80,histtype='step',range=(0,10))\n",
    "# plt.hist(background[background['reco_MX_mgg']>-5],bins=80,histtype='step',range=(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15f21a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
